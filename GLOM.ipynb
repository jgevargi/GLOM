{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 40\n",
    "batch_size_test = 40\n",
    "learning_rate = 0.01\n",
    "log_interval = 10\n",
    "\n",
    "num_vectors = 4\n",
    "len_vectors = 10\n",
    "img_height = 28\n",
    "img_width = 28\n",
    "win_size = 3\n",
    "epsilon = .7\n",
    "epochs = 4000\n",
    "steps = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loader(train_val, batch_size):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            '/files/', \n",
    "            train = train_val, \n",
    "            download = True,                   \n",
    "            transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "             ])\n",
    "        ),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = build_loader(true, batch_size_train)\n",
    "\n",
    "test_loader = build_loader(false, batch_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_state(example_data, batch_size):\n",
    "    # reshape MNIST input image and stack image data until \n",
    "    # the bottom state vector has the entire vector filled\n",
    "    temp_example_data = torch.reshape(\n",
    "        example_data,\n",
    "        (batch_size, img_height, img_width)\n",
    "    )\n",
    "    temp_inp = [temp_example_data for i in range(10)]\n",
    "    temp_inp_data = torch.stack((temp_inp), dim = 3)\n",
    "    return temp_inp_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targets_to_state(example_targets, batch_size):\n",
    "    # one hot MNIST target data and uniformly spread correct target vector \n",
    "    # across every pixel of the state per batch\n",
    "    return torch.nn.functional\n",
    "        .one_hot(example_targets, num_classes=10)\n",
    "        .repeat(1, img_height*img_width)\n",
    "        .view((batch_size, img_height, img_width, 10))\n",
    "        .float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CA Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_state(batch_size, img_height, img_width, num_vectors, len_vectors):\n",
    "    state = torch.rand(\n",
    "        (batch_size, img_height, img_width, num_vectors, len_vectors)\n",
    "    ) * .1\n",
    "    return state.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, num_inp, num_out):\n",
    "        super(model, self).__init__()\n",
    "        self.Q1 = nn.Linear(num_inp, num_out)\n",
    "        self.K1 = nn.Linear(num_out, num_out)\n",
    "        self.V1 = nn.Linear(num_out, num_out)\n",
    "        \n",
    "        self.m = nn.Dropout(p = 0.1)\n",
    "        \n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.act3 = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        Q = self.act1(self.Q1(self.m(x)))\n",
    "        K = self.act1(self.K1(self.m(Q))) + Q\n",
    "        V = self.act1(self.V1(self.m(K))) + Q + K\n",
    "        \n",
    "        return V * .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_attention(center_matrix,roll_matrix,after_tri):\n",
    "    # dot product vectors to find similarity of adjacent vectors\n",
    "    after_mul = torch.matmul(\n",
    "        center_matrix, \n",
    "        roll_matrix.permute((0, 1, 2, 4, 3))\n",
    "    )\n",
    "    after_diag = torch.diagonal(after_mul, offset = 0, dim1 = 3, dim2 = 4)\n",
    "    \n",
    "    # multiply vectors by lambda matrix to find full attention numbers\n",
    "    after_eps = torch.matmul(after_diag, after_tri)\n",
    "    \n",
    "    # stack full attention numbers so that each vector gets\n",
    "    # its proper attention number\n",
    "    after_sim = torch.stack(\n",
    "        [after_eps for i in range(len_vectors)],\n",
    "        dim=3\n",
    "    ).permute((0,1,2,4,3))\n",
    "    \n",
    "    # multiply each vector by the attention numbers to complete\n",
    "    # the attention step\n",
    "    full_vec_dis = center_matrix * after_sim.detach()\n",
    "    \n",
    "    return full_vec_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: function too long. Complexity too high\n",
    "def compute_all(\n",
    "    bottom_up_model_list, top_down_model_list, layer_att_model_list, \n",
    "    state, len_vectors, num_vectors, batch_size\n",
    "):\n",
    "    # shift state to in 9 directions along the x and y plane\n",
    "    shifts_list = [\n",
    "        (-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 0),\n",
    "        (0, 1), (1, -1), (1, 0), (1, 1)\n",
    "    ]\n",
    "    \n",
    "    roll_list = map(\n",
    "        lambda shifts_val: torch.roll(\n",
    "            state, shifts = shifts_val, dims = (1, 2)\n",
    "        ).to(device),\n",
    "        shifts_list\n",
    "    ) \n",
    "    \n",
    "    eps_matrix = epsilon ** torch.arange(start = 1, end = num_vectors+1)\n",
    "    try_roll = [\n",
    "        torch.roll(eps_matrix, shifts=(i), dims = (0))\n",
    "        for i in range(eps_matrix.shape[0])\n",
    "    ]\n",
    "    try_roll = torch.stack(try_roll)\n",
    "    after_tri = torch.triu(try_roll, diagonal = 0).T.to(device)\n",
    "    \n",
    "    att_list = []\n",
    "    for roll in roll_list:\n",
    "        att_list.append(get_layer_attention(roll, roll5, after_tri))\n",
    "    \n",
    "    # concatenate vectors so that att_list contains the state and\n",
    "    # every adjacent vector on the same vector level\n",
    "    att_list = torch.cat(roll_list, dim=4)\n",
    "    \n",
    "    # feed layers to models:\n",
    "    # top-down models don't get first two layers as input and \n",
    "    # don't add to 1st & last layer\n",
    "    # bot-up models don't get last layer as input and don't add to first layer \n",
    "    # adjacent models don't add to first layer\n",
    "    delta = [\n",
    "        torch.zeros(\n",
    "            (batch_size*img_height*img_width, len_vectors)\n",
    "        ).to(device) \n",
    "        for i in range(num_vectors)\n",
    "    ]\n",
    "    for i in range(num_vectors):\n",
    "        if(i<num_vectors-2):\n",
    "            top_down_temp = top_down_model_list[i](\n",
    "                torch.reshape(\n",
    "                    att_list[:, :, :, i+2, :], \n",
    "                    (-1, len_vectors*9)\n",
    "                )\n",
    "            )\n",
    "            delta[i+1] = delta[i+1] + top_down_temp\n",
    "        if(i<num_vectors-1):\n",
    "            bottom_up_temp = bottom_up_model_list[i](\n",
    "                torch.reshape(\n",
    "                    att_list[:, :, :, i, :],\n",
    "                    (-1, len_vectors*9)\n",
    "                )\n",
    "            )\n",
    "            att_layer_temp = layer_att_model_list[i](\n",
    "                torch.reshape(\n",
    "                    att_list[:, :, :, i+1, :],\n",
    "                    (-1, len_vectors*9)\n",
    "                )\n",
    "            )\n",
    "            delta[i+1] = delta[i+1] + bottom_up_temp + att_layer_temp\n",
    "    \n",
    "    #format delta so that delta and state can be added together\n",
    "    delta = torch.stack(delta, dim=1)#.permute(0, 2, 1)#.permute(2, 0, 1)\n",
    "    delta = torch.reshape(\n",
    "        delta,\n",
    "        (batch_size, img_height, img_width, num_vectors, len_vectors)\n",
    "    )\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_up_model_list = [model(9*len_vectors,len_vectors).cuda() for i in range(num_vectors-1)]\n",
    "top_down_model_list= [model(9*len_vectors,len_vectors).cuda() for i in range(num_vectors-2)]\n",
    "layer_att_model_list = [model(9*len_vectors,len_vectors).cuda() for i in range(num_vectors-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create parameter list of every model to feed into optimizer\n",
    "param_list = []\n",
    "for i in range(num_vectors):\n",
    "    if (i<num_vectors - 2):\n",
    "        param_list =  param_list + list(top_down_model_list[i].parameters())\n",
    "    if (i<num_vectors-1):\n",
    "        param_list =  param_list + list(bottom_up_model_list[i].parameters())\n",
    "        param_list =  param_list + list(layer_att_model_list[i].parameters())\n",
    "        \n",
    "optimizer = torch.optim.Adam(param_list, lr=learning_rate)\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # in case StopIteration error is raised\n",
    "    try:\n",
    "        batch_idx, (example_data, example_targets) = next(examples)\n",
    "    except StopIteration:\n",
    "        examples = enumerate(train_loader)\n",
    "        batch_idx, (example_data, example_targets) = next(examples)\n",
    "    \n",
    "    # initialize state\n",
    "    state = init_state(\n",
    "        batch_size_train, img_height, img_width, num_vectors, len_vectors\n",
    "    )\n",
    "    \n",
    "    # put current batches into state\n",
    "    state[:, :, :, 0, :] = data_to_state(example_data, batch_size_train)\n",
    "    state1 = torch.clone(state)\n",
    "    state2 = torch.clone(state)\n",
    "    for step in range(steps):\n",
    "        \n",
    "        delta = compute_all(\n",
    "            bottom_up_model_list, top_down_model_list, layer_att_model_list, \n",
    "            state, len_vectors, num_vectors, batch_size_train\n",
    "        )\n",
    "        \n",
    "        state = state + delta + .0001 * torch.rand((state.shape)).to(device)\n",
    "        \n",
    "        # add first state to state in the middle of the steps\n",
    "        # (allows for RESNET type gradient backprop)\n",
    "        if ( step % int(steps/2) == 0):\n",
    "            state = state + state1 * .1\n",
    "            state1 = torch.clone(state)\n",
    "            \n",
    "        if (step % int(steps/4) == 0):\n",
    "            state = state + state2 * .1\n",
    "            state2 = torch.clone(state)\n",
    "            \n",
    "    state = state + state1 * .1\n",
    "    # get loss\n",
    "    pred_out = state[:, :, :, -1]\n",
    "    targ_out = targets_to_state(example_targets, batch_size_train)\n",
    "    loss = mse(pred_out, targ_out)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Epoch: {}/{}  Loss: {}\".format(epoch, epochs, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tot_corr = 0\n",
    "tot_batches = 0\n",
    "for example_data, target in test_loader:\n",
    "    tot_batches+=batch_size_test\n",
    "    \n",
    "    #initialize state\n",
    "    state = init_state(\n",
    "        batch_size_test, img_height, img_width, num_vectors, len_vectors\n",
    "    )\n",
    "    \n",
    "    #put current batches into state\n",
    "    state[:, :, :, 0, :] = data_to_state(example_data,batch_size_test)\n",
    "    for step in range(steps):\n",
    "        delta = compute_all(\n",
    "            bottom_up_model_list, top_down_model_list, layer_att_model_list,\n",
    "            state, len_vectors, num_vectors ,batch_size_test\n",
    "        )\n",
    "        \n",
    "        #update state\n",
    "        state = state + delta\n",
    "        \n",
    "    for batch in range(batch_size_test):\n",
    "        temp = torch.zeros((10))\n",
    "        for height in range(img_height):\n",
    "            for width in range(img_width):\n",
    "                ind = torch.argmax(state[batch, height, width, -1])\n",
    "                temp[ind] += 1\n",
    "        \n",
    "        if(target[batch] == torch.argmax(temp)):\n",
    "            tot_corr += 1\n",
    "    print(\"Acc: {}\".format(tot_corr/tot_batches))\n",
    "\n",
    "print(\"Final Accuracy: {}\".format(tot_corr/tot_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(\n",
    "    bottom_up_model_list, top_down_model_list, layer_att_model_list\n",
    "):\n",
    "    PATH = \"best_models/\"\n",
    "    for i in range(num_vectors):\n",
    "        if (i < num_vectors - 2):\n",
    "            torch.save(\n",
    "                top_down_model_list[i],\n",
    "                PATH + \"top_down_model{}\".format(i)\n",
    "            )\n",
    "        if (i < num_vectors - 1):\n",
    "            torch.save(\n",
    "                bottom_up_model_list[i],\n",
    "                PATH + \"bottom_up_model{}\".format(i)\n",
    "            )\n",
    "            torch.save(\n",
    "                layer_att_model_list[i],\n",
    "                PATH + \"layer_att_model{}\".format(i)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models(bottom_up_model_list, top_down_model_list, layer_att_model_list)"
   ]
  }
 ],
 "metadata": {
  "julynter-check-restart": true,
  "julynter-results": {
   "filteredId": [],
   "filteredIndividual": [],
   "filteredRestart": [],
   "filteredType": [],
   "hash": "301cac72810c481b3c51e0405a372f834e3d2201",
   "visible": [
    {
     "cellId": "group",
     "hash": "19d3d3d153f22e318cbe061df9e685ba5165b0aa",
     "reason": "This groups other lint messages",
     "reportId": "group",
     "reportType": "confusenotebook",
     "suggestion": null,
     "text": "Confuse Notebook"
    },
    {
     "cellId": 0,
     "hash": "9206849ba0c71754b08cc17abe98534e6b9ba251",
     "reason": "A markdown cell at the beginning of the notebook can provide a human-friendly title with no constraints and introduce the notebook, indicating its purpose and external requirements.",
     "reportId": "c4",
     "reportType": "confusenotebook",
     "suggestion": "Please consider adding a markdown cell to describe the notebook.",
     "text": "The first cell of the notebook is not a markdown cell"
    },
    {
     "cellId": 21,
     "hash": "3dd6f79ecc33d7a667e5540d530d374f7fd4fa06",
     "reason": "A markdown cell at the end of the notebook can conclude it, presenting a summary of the obtained results.",
     "reportId": "c5",
     "reportType": "confusenotebook",
     "suggestion": "Please consider adding a markdown cell to conclude the notebook.",
     "text": "The last cell of the notebook is not a markdown cell"
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
